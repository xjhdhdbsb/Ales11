from transformers import GPT2Tokenizer, GPT2LMHeadModel, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel
import torch
import warnings
warnings.filterwarnings("ignore")

class GPTChat:
    def __init__(self, model_type="gpt2"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Ç-–±–æ—Ç–∞
        model_type: "gpt1" –∏–ª–∏ "gpt2"
        """
        self.model_type = model_type
        
        if model_type == "gpt1":
            print("ü§ñ –ó–∞–≥—Ä—É–∂–∞—é GPT-1...")
            self.tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
            self.model = OpenAIGPTLMHeadModel.from_pretrained("openai-gpt")
        elif model_type == "gpt2":
            print("ü§ñ –ó–∞–≥—Ä—É–∂–∞—é GPT-2 Medium...")
            self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
            self.model = GPT2LMHeadModel.from_pretrained("gpt2-medium")
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –¥–ª—è GPT-2
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.max_length = 1000  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤
        self.temperature = 0.8
        self.top_k = 50
        self.top_p = 0.9
        
        print(f"‚úÖ {model_type.upper()} –≥–æ—Ç–æ–≤ –∫ –æ–±—â–µ–Ω–∏—é!")
        self.show_commands()
        
        # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        self.conversation_history = ""

    def show_commands(self):
        print("üí° –ö–æ–º–∞–Ω–¥—ã:")
        print("   /settings - –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        print("   /clear - –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
        print("   /info - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
        print("   /quit –∏–ª–∏ /exit - –≤—ã–π—Ç–∏")
        print("=" * 50)

    def get_model_info(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        if self.model_type == "gpt1":
            return """
üìä GPT-1 (2018):
‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 117M
‚Ä¢ –°–ª–æ–µ–≤: 12
‚Ä¢ –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: 768
‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: 512 —Ç–æ–∫–µ–Ω–æ–≤
‚Ä¢ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: –ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤–∞, –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer
‚Ä¢ –û–±—É—á–µ–Ω–∏–µ: –ù–µ—Å—É–ø–µ—Ä–µ–≤–∏–∑–æ—Ä–Ω–æ–µ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ BookCorpus
            """
        else:
            return """
üìä GPT-2 Medium (2019):
‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: 355M
‚Ä¢ –°–ª–æ–µ–≤: 24
‚Ä¢ –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: 1024
‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: 1024 —Ç–æ–∫–µ–Ω–∞
‚Ä¢ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è GPT-2, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —á–µ–º Small
‚Ä¢ –û–±—É—á–µ–Ω–∏–µ: 40GB —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ (WebText)
            """

    def generate_response(self, user_input):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–≤–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            if self.conversation_history:
                prompt = f"{self.conversation_history}\nHuman: {user_input}\nAssistant:"
            else:
                prompt = f"Human: {user_input}\nAssistant:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å attention_mask
            encoded = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512 if self.model_type == "gpt1" else 800
            )
            
            inputs = encoded['input_ids']
            attention_mask = encoded.get('attention_mask', None)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            max_context = 400 if self.model_type == "gpt1" else 600
            if inputs.size(1) > max_context:
                inputs = inputs[:, -max_context:]
                if attention_mask is not None:
                    attention_mask = attention_mask[:, -max_context:]
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            max_new_tokens = 150 if self.model_type == "gpt1" else 200
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    top_p=self.top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2,
                    repetition_penalty=1.1,
                    # –£–±–∏—Ä–∞–µ–º early_stopping=True —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –±–æ—Ç–∞
            prompt_text = self.tokenizer.decode(inputs[0], skip_special_tokens=True)
            bot_response = full_response[len(prompt_text):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —á–∞—Å—Ç–µ–π
            for separator in ["\nHuman:", "\nAssistant:", "\n\n", "Human:", "Assistant:"]:
                if separator in bot_response:
                    bot_response = bot_response.split(separator)[0].strip()
            
            # –£–¥–∞–ª—è–µ–º –∫–∞–≤—ã—á–∫–∏ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –≤ –Ω–∏—Ö –æ–±–µ—Ä–Ω—É—Ç
            if bot_response.startswith('"') and bot_response.endswith('"'):
                bot_response = bot_response[1:-1].strip()
            
            # –û–±—Ä–µ–∑–∞–µ–º –æ—Ç–≤–µ—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (—Ç–µ–ø–µ—Ä—å 1000 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(bot_response) > self.max_length:
                truncated = bot_response[:self.max_length]
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                last_sentence_end = max(
                    truncated.rfind('.'),
                    truncated.rfind('!'),
                    truncated.rfind('?')
                )
                if last_sentence_end > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                    bot_response = truncated[:last_sentence_end + 1]
                else:
                    bot_response = truncated.rstrip() + "..."
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return bot_response if bot_response else "I'm not sure how to respond to that. Could you try rephrasing?"
            
        except Exception as e:
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

    def show_settings(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        print(f"\n‚öôÔ∏è –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ({self.model_type.upper()}):")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {self.max_length} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å): {self.temperature}")
        print(f"   Top-k —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.top_k}")
        print(f"   Top-p —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.top_p}")
        print()

    def change_settings(self):
        """–ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        self.show_settings()
        
        try:
            new_temp = input(f"–ù–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.1-2.0, —Ç–µ–∫—É—â–∞—è {self.temperature:.1f}): ")
            if new_temp.strip():
                self.temperature = max(0.1, min(2.0, float(new_temp)))
            
            new_length = input(f"–ù–æ–≤–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (50-2000, —Ç–µ–∫—É—â–∞—è {self.max_length}): ")
            if new_length.strip():
                self.max_length = max(50, min(2000, int(new_length)))
            
            new_top_k = input(f"–ù–æ–≤—ã–π top-k (1-100, —Ç–µ–∫—É—â–∏–π {self.top_k}): ")
            if new_top_k.strip():
                self.top_k = max(1, min(100, int(new_top_k)))
            
            new_top_p = input(f"–ù–æ–≤—ã–π top-p (0.1-1.0, —Ç–µ–∫—É—â–∏–π {self.top_p:.1f}): ")
            if new_top_p.strip():
                self.top_p = max(0.1, min(1.0, float(new_top_p)))
            
            print("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")
            self.show_settings()
            
        except ValueError:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã.")

    def run_chat(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á–∞—Ç–∞"""
        while True:
            try:
                user_input = input(f"\nüë§ –í—ã: ").strip()
                
                if not user_input:
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—ã
                if user_input.lower() in ['/quit', '/exit', 'quit', 'exit']:
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break
                
                elif user_input.lower() == '/clear':
                    self.conversation_history = ""
                    print("üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
                    continue
                
                elif user_input.lower() == '/settings':
                    self.change_settings()
                    continue
                
                elif user_input.lower() == '/info':
                    print(self.get_model_info())
                    continue
                
                elif user_input.lower() == '/help':
                    self.show_commands()
                    continue
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                print(f"ü§ñ {self.model_type.upper()} –¥—É–º–∞–µ—Ç...", end="", flush=True)
                response = self.generate_response(user_input)
                print(f"\rü§ñ {self.model_type.upper()}: {response}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
                self.conversation_history += f"\nHuman: {user_input}\nAssistant: {response}"
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (—É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤)
                max_history = 1000 if self.model_type == "gpt1" else 1500
                if len(self.conversation_history) > max_history:
                    self.conversation_history = self.conversation_history[-max_history:]
                
            except KeyboardInterrupt:
                print(f"\n\nüëã –ß–∞—Ç —Å {self.model_type.upper()} –ø—Ä–µ—Ä–≤–∞–Ω. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏"""
    print("üöÄ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ GPT Chat!")
    print("\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:")
    print("1. GPT-1 (117M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 2018)")
    print("2. GPT-2 Medium (355M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 2019)")
    
    while True:
        choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1/2): ").strip()
        
        if choice == "1":
            model_type = "gpt1"
            break
        elif choice == "2":
            model_type = "gpt2"
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2.")
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —á–∞—Ç
    chat = GPTChat(model_type)
    chat.run_chat()

if __name__ == "__main__":
    main()
